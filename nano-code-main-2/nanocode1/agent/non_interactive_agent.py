import asyncio
import json
import os
from typing import List, Dict, Any
from pathlib import Path
from rich.console import Console
from rich.panel import Panel
from rich.markdown import Markdown as M

from ..core.session import Session
from ..llm import llm_complete
from ..agent_tool.tools import OS_TOOLS, UTIL_TOOLS, PYTHON_TOOLS, GIT_TOOLS
from ..utils.logger import AIConsoleLogger
from ..prompts import SYSTEM_PROMPT, RAW_ANALYSIS_PROMPT
from ..models.dissertation_plan import DissertationPlan


class NonInteractiveAgent:
    
    def __init__(self, session: Session, console: Console = None):
        self.session = session
        self.console = console or Console()
        self.all_tools = OS_TOOLS.merge(UTIL_TOOLS).merge(PYTHON_TOOLS).merge(GIT_TOOLS)
        self.execution_log = []
    
    
    async def execute_task_intelligently(self, task_context: dict) -> Dict[str, Any]:
        input_type = task_context.get("type")
        
        if input_type == "url_analysis":
            return await self._execute_url_analysis(task_context)
        elif input_type == "json_task_execution":
            return await self._execute_json_tasks(task_context)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {input_type}")
    
    
    async def _execute_url_analysis(self, task_context: dict) -> Dict[str, Any]: #åˆ†æä»£ç ä»“åº“
        url = task_context["url"]
        
        analysis_prompt = f"è¯·åˆ†æä»¥ä¸‹ä»£ç ä»“åº“ï¼š{url}"
        messages = [{"role": "user", "content": analysis_prompt}]
        
        result = await self._autonomous_execution_loop(
            messages, 
            analysis_prompt,
            system_prompt=RAW_ANALYSIS_PROMPT
        )
        
        return {
            "status": "completed",
            "phase": "url_analysis",
            "url": url,
            "analysis_document": result.get("final_message", ""),
            "iteration": result.get("iteration", 0),
            "execution_log": result.get("execution_log", [])
        }
    
    
    async def _execute_json_tasks(self, task_context: dict) -> Dict[str, Any]:
        dissertation_plan = task_context["dissertation_plan"]
        
        task_prompt = self._convert_dissertation_plan_to_prompt(dissertation_plan)
        messages = [{"role": "user", "content": task_prompt}]
        
        result = await self._autonomous_execution_loop(
            messages,
            task_prompt,
            system_prompt=SYSTEM_PROMPT
        )
        
        return {
            "status": "completed",
            "phase": "json_task_execution", 
            "task_results": result.get("final_message", ""),
            "iteration": result.get("iteration", 0),
            "execution_log": result.get("execution_log", [])
        }
    
    
    def _convert_dissertation_plan_to_prompt(self, plan: DissertationPlan) -> str:
        """å°†DissertationPlanè½¬æ¢ä¸ºAgentå¯æ‰§è¡Œçš„æç¤º."""
        prompt_parts = [
            f"# å­¦æœ¯ç ”ç©¶ä»»åŠ¡ï¼š{plan.dissertation_title}",
            "",
            "## ç ”ç©¶èƒŒæ™¯",
            f"æ–‡çŒ®ä¸»é¢˜ï¼š{', '.join(plan.literature_topic)}",
            "",
            "## éœ€è¦æ‰§è¡Œçš„ç ”ç©¶å†…å®¹",
        ]
        
        # ä»£ç ä»“åº“åˆ†æéƒ¨åˆ†
        if plan.experimental_requirements.code_repository_review:
            repo = plan.experimental_requirements.code_repository_review
            prompt_parts.extend([
                "### ä»£ç ä»“åº“åˆ†æ",
                f"- ä»“åº“åœ°å€ï¼š{repo.url}",
                f"- æè¿°ï¼š{repo.description}",
                f"- åˆ†æé‡ç‚¹ï¼š{', '.join(repo.analysis_focus)}",
                ""
            ])
        
        # å®éªŒä»»åŠ¡éƒ¨åˆ†
        if plan.experimental_requirements.reproduction_tasks:
            prompt_parts.append("### éœ€è¦å®Œæˆçš„å®éªŒä»»åŠ¡")
            for i, task in enumerate(plan.experimental_requirements.reproduction_tasks, 1):
                prompt_parts.extend([
                    f"{i}. **{task.phase}**",
                    f"   - ç›®æ ‡ï¼š{task.target}",
                    f"   - æ–¹æ³•ï¼š{task.methodology}",
                    ""
                ])
        
        # è¯„ä¼°è¦æ±‚
        if plan.experimental_requirements.critical_evaluation:
            eval_req = plan.experimental_requirements.critical_evaluation
            prompt_parts.extend([
                "### æ‰¹åˆ¤æ€§è¯„ä¼°è¦æ±‚",
                f"- å¤±è´¥æ¡ˆä¾‹ç ”ç©¶ï¼š{eval_req.failure_case_study}",
                f"- æ”¹è¿›æ–¹å‘ï¼š{', '.join(eval_req.improvement_directions)}",
                ""
            ])
        
        # ç›¸å…³èµ„æº
        if plan.urls:
            prompt_parts.append("### ç›¸å…³èµ„æº")
            for url_info in plan.urls:
                prompt_parts.append(f"- {url_info.url}: {url_info.description}")
            prompt_parts.append("")
        
        # æ‰§è¡ŒæŒ‡å¯¼
        prompt_parts.extend([
            "## æ‰§è¡Œè¦æ±‚",
            "è¯·ä½œä¸ºä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œæ™ºèƒ½åœ°åˆ†æä¸Šè¿°ç ”ç©¶è®¡åˆ’ï¼Œå¹¶ï¼š",
            "1. è‡ªä¸»å†³å®šæœ€ä½³çš„æ‰§è¡Œé¡ºåºå’Œæ–¹æ³•",
            "2. çµæ´»ä½¿ç”¨å¯ç”¨çš„å·¥å…·å®Œæˆå„é¡¹ç ”ç©¶ä»»åŠ¡", 
            "3. æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ç ”ç©¶ç­–ç•¥",
            "4. ç”Ÿæˆé«˜è´¨é‡çš„ç ”ç©¶è¾“å‡ºå’Œæ–‡æ¡£",
            "",
            "ä½ æœ‰å®Œå…¨çš„è‡ªä¸»æƒæ¥å†³å®šå¦‚ä½•æœ€å¥½åœ°å®Œæˆè¿™ä¸ªç ”ç©¶è®¡åˆ’ã€‚"
        ])
        
        return "\n".join(prompt_parts)
    
    async def _autonomous_execution_loop(
        self, 
        messages: List[dict], 
        prompt_content: str, 
        system_prompt: str
    ) -> Dict[str, Any]:
        """è‡ªä¸»æ‰§è¡Œå¾ªç¯çš„æ ¸å¿ƒé€»è¾‘."""
        iteration = 0
        
        # è·å–é¡¹ç›®å†…å­˜
        code_memories = self.session.get_memory()
        memories = f"""Below are some working memories:
{code_memories}""" if code_memories else ""
        
        while True:
            iteration += 1
            self.console.print(f"ğŸ”„ æ‰§è¡Œè½®æ¬¡ {iteration}")
            
            # è°ƒç”¨LLM
            response = await llm_complete(
                self.session,
                self.session.working_env.llm_main_model,
                messages,
                system_prompt=system_prompt.format(
                    working_dir=self.session.working_dir,
                    memories=memories
                ),
                tools=self.all_tools.get_schemas(),
            )
            
            choice = response.choices[0]
            
            if choice.finish_reason != "tool_calls":
                self.console.print("âœ… ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
                return {
                    "status": "completed",
                    "final_message": choice.message.content,
                    "iteration": iteration,
                    "execution_log": self.execution_log
                }
            
            # æ˜¾ç¤ºAIçš„æ€è€ƒè¿‡ç¨‹
            if choice.message.content:
                # è½¬ä¹‰Rich Consoleæ ‡è®°ä»¥é¿å…è§£æé”™è¯¯
                safe_content = choice.message.content.replace('[', '\\[').replace(']', '\\]')
                self.console.print(Panel(M(safe_content), title="Assistant"))
            
            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
            messages.append(choice.message.model_dump())

            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_calls = [
                t for t in choice.message.tool_calls
                if self.all_tools.has_tool(t.function.name)
            ]
            
            # å·¥å…·è°ƒç”¨ä¿¡æ¯
            for t in tool_calls:
                self.console.print(f"ğŸ”§ [bold blue]è°ƒç”¨å·¥å…·:[/bold blue] {t.function.name}")
                try:
                    args = json.loads(t.function.arguments)
                    self.console.print(f"ğŸ“ [bold green]å‚æ•°:[/bold green] {json.dumps(args, indent=2, ensure_ascii=False)}")
                except json.JSONDecodeError:
                    self.console.print(f"ğŸ“ [bold yellow]å‚æ•° (åŸå§‹):[/bold yellow] {t.function.arguments}")
                self.console.print("â”€" * 50)
            
            # æ‰¹é‡æ‰§è¡Œå·¥å…·
            tasks = [
                self.all_tools.execute(
                    self.session, t.function.name, json.loads(t.function.arguments)
                )
                for t in tool_calls
            ]
            
            results = await asyncio.gather(*tasks)
            
            # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
            for t, r in zip(tool_calls, results):
                messages.append({
                    "role": "tool",
                    "tool_call_id": t.id,
                    "content": r.for_llm,
                })
                
                # è®°å½•æ‰§è¡Œæ—¥å¿—
                self.execution_log.append({
                    "iteration": iteration,
                    "tool": t.function.name,
                    "args": json.loads(t.function.arguments),
                    "result": r.for_human
                })
        


async def run_intelligent_task(task_context: dict, working_dir: str = None):
    """æ–°çš„ç»Ÿä¸€ä»»åŠ¡æ‰§è¡Œå…¥å£å‡½æ•°."""
    console = Console()
    
    # è®¾ç½®å·¥ä½œç›®å½•
    if working_dir is None:
        working_dir = os.getcwd()
    
    # åˆ›å»ºä¼šè¯
    session = Session(working_dir=working_dir, logger=AIConsoleLogger(console))
    
    agent = NonInteractiveAgent(session, console)
    
    try:
        console.print("ğŸš€ Agentå¼€å§‹æ‰§è¡Œä»»åŠ¡...")
        
        result = await agent.execute_task_intelligently(task_context)
        
        console.print(Panel(
            f"çŠ¶æ€: {result['status']}\n"
            f"æ‰§è¡Œé˜¶æ®µ: {result.get('phase', 'unknown')}\n"
            f"ä½¿ç”¨è½®æ¬¡: {result.get('iteration', 0)}\n"
            f"æ‰§è¡Œæ­¥éª¤: {len(result.get('execution_log', []))} ä¸ª",
            title="ğŸ“Š ä»»åŠ¡æ‰§è¡Œæ‘˜è¦",
            border_style="green" if result['status'] == 'completed' else "yellow"
        ))
        
        return result
        
    finally:
        # ä¿å­˜æ£€æŸ¥ç‚¹
        session.save_checkpoints()